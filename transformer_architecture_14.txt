[('',
  VisionTransformer(
  (conv_proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
  (encoder): Encoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (encoder_layer_0): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_1): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_2): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_3): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_4): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_5): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_6): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_7): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_8): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_9): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_10): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_11): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_12): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_13): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_14): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_15): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_16): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_17): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_18): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_19): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_20): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_21): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_22): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_23): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_24): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_25): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_26): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_27): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_28): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_29): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_30): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_31): EncoderBlock(
        (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=1280, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=5120, out_features=1280, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  )
  (heads): Sequential(
    (head): Linear(in_features=1280, out_features=1000, bias=True)
  )
)),
 ('conv_proj', Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))),
 ('encoder',
  Encoder(
  (dropout): Dropout(p=0.0, inplace=False)
  (layers): Sequential(
    (encoder_layer_0): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_1): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_2): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_3): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_4): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_5): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_6): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_7): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_8): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_9): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_10): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_11): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_12): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_13): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_14): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_15): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_16): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_17): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_18): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_19): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_20): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_21): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_22): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_23): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_24): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_25): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_26): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_27): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_28): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_29): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_30): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
    (encoder_layer_31): EncoderBlock(
      (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLPBlock(
        (0): Linear(in_features=1280, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Dropout(p=0.0, inplace=False)
        (3): Linear(in_features=5120, out_features=1280, bias=True)
        (4): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (ln): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
)),
 ('encoder.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers',
  Sequential(
  (encoder_layer_0): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_1): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_2): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_3): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_4): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_5): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_6): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_7): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_8): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_9): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_10): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_11): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_12): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_13): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_14): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_15): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_16): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_17): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_18): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_19): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_20): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_21): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_22): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_23): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_24): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_25): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_26): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_27): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_28): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_29): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_30): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
  (encoder_layer_31): EncoderBlock(
    (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (self_attention): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    (mlp): MLPBlock(
      (0): Linear(in_features=1280, out_features=5120, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.0, inplace=False)
      (3): Linear(in_features=5120, out_features=1280, bias=True)
      (4): Dropout(p=0.0, inplace=False)
    )
  )
)),
 ('encoder.layers.encoder_layer_0',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_0.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_0.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_0.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_0.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_0.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_0.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_0.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_0.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_0.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_0.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_0.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_1',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_1.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_1.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_1.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_1.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_1.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_1.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_1.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_1.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_1.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_1.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_1.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_2',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_2.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_2.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_2.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_2.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_2.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_2.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_2.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_2.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_2.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_2.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_2.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_3',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_3.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_3.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_3.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_3.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_3.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_3.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_3.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_3.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_3.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_3.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_3.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_4',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_4.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_4.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_4.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_4.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_4.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_4.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_4.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_4.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_4.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_4.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_4.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_5',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_5.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_5.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_5.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_5.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_5.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_5.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_5.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_5.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_5.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_5.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_5.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_6',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_6.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_6.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_6.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_6.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_6.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_6.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_6.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_6.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_6.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_6.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_6.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_7',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_7.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_7.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_7.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_7.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_7.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_7.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_7.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_7.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_7.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_7.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_7.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_8',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_8.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_8.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_8.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_8.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_8.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_8.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_8.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_8.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_8.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_8.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_8.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_9',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_9.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_9.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_9.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_9.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_9.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_9.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_9.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_9.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_9.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_9.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_9.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_10',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_10.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_10.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_10.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_10.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_10.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_10.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_10.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_10.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_10.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_10.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_10.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_11',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_11.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_11.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_11.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_11.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_11.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_11.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_11.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_11.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_11.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_11.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_11.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_12',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_12.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_12.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_12.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_12.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_12.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_12.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_12.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_12.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_12.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_12.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_12.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_13',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_13.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_13.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_13.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_13.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_13.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_13.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_13.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_13.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_13.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_13.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_13.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_14',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_14.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_14.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_14.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_14.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_14.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_14.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_14.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_14.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_14.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_14.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_14.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_15',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_15.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_15.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_15.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_15.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_15.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_15.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_15.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_15.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_15.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_15.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_15.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_16',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_16.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_16.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_16.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_16.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_16.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_16.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_16.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_16.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_16.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_16.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_16.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_17',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_17.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_17.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_17.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_17.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_17.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_17.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_17.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_17.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_17.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_17.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_17.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_18',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_18.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_18.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_18.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_18.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_18.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_18.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_18.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_18.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_18.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_18.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_18.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_19',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_19.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_19.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_19.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_19.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_19.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_19.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_19.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_19.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_19.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_19.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_19.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_20',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_20.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_20.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_20.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_20.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_20.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_20.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_20.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_20.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_20.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_20.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_20.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_21',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_21.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_21.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_21.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_21.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_21.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_21.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_21.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_21.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_21.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_21.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_21.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_22',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_22.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_22.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_22.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_22.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_22.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_22.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_22.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_22.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_22.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_22.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_22.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_23',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_23.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_23.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_23.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_23.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_23.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_23.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_23.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_23.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_23.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_23.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_23.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_24',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_24.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_24.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_24.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_24.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_24.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_24.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_24.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_24.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_24.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_24.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_24.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_25',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_25.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_25.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_25.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_25.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_25.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_25.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_25.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_25.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_25.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_25.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_25.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_26',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_26.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_26.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_26.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_26.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_26.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_26.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_26.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_26.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_26.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_26.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_26.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_27',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_27.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_27.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_27.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_27.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_27.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_27.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_27.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_27.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_27.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_27.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_27.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_28',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_28.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_28.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_28.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_28.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_28.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_28.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_28.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_28.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_28.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_28.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_28.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_29',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_29.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_29.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_29.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_29.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_29.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_29.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_29.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_29.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_29.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_29.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_29.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_30',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_30.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_30.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_30.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_30.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_30.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_30.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_30.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_30.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_30.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_30.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_30.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_31',
  EncoderBlock(
  (ln_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (self_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (ln_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
  (mlp): MLPBlock(
    (0): Linear(in_features=1280, out_features=5120, bias=True)
    (1): GELU(approximate='none')
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=5120, out_features=1280, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
)),
 ('encoder.layers.encoder_layer_31.ln_1',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_31.self_attention',
  MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
)),
 ('encoder.layers.encoder_layer_31.self_attention.out_proj',
  NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_31.dropout', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_31.ln_2',
  LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('encoder.layers.encoder_layer_31.mlp',
  MLPBlock(
  (0): Linear(in_features=1280, out_features=5120, bias=True)
  (1): GELU(approximate='none')
  (2): Dropout(p=0.0, inplace=False)
  (3): Linear(in_features=5120, out_features=1280, bias=True)
  (4): Dropout(p=0.0, inplace=False)
)),
 ('encoder.layers.encoder_layer_31.mlp.0',
  Linear(in_features=1280, out_features=5120, bias=True)),
 ('encoder.layers.encoder_layer_31.mlp.1', GELU(approximate='none')),
 ('encoder.layers.encoder_layer_31.mlp.2', Dropout(p=0.0, inplace=False)),
 ('encoder.layers.encoder_layer_31.mlp.3',
  Linear(in_features=5120, out_features=1280, bias=True)),
 ('encoder.layers.encoder_layer_31.mlp.4', Dropout(p=0.0, inplace=False)),
 ('encoder.ln', LayerNorm((1280,), eps=1e-06, elementwise_affine=True)),
 ('heads',
  Sequential(
  (head): Linear(in_features=1280, out_features=1000, bias=True)
)),
 ('heads.head', Linear(in_features=1280, out_features=1000, bias=True))]
